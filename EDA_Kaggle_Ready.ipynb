{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro_markdown",
      "metadata": {},
      "source": [
        "# Sensor Fault Detection: EDA and Modeling\n",
        "\n",
        "This notebook performs an exploratory data analysis (EDA) and evaluates multiple machine learning models for the Sensor Fault Detection dataset. \n",
        "\n",
        "**Enhancements for Kaggle:**\n",
        "*   All required libraries are installed in the first cell.\n",
        "*   Computationally expensive steps like data imputation and resampling are cached. The notebook saves the results of these steps and loads them on subsequent runs to save time.\n",
        "*   Code errors from the original file have been fixed and highlighted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install_libraries",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# This cell installs all required libraries for the notebook to run on Kaggle.\n",
        "# The -q flag is used for a quieter installation output.\n",
        "!pip install -q xgboost catboost scikit-learn imbalanced-learn miceforest kneed prettytable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "consolidated_imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# All imports are consolidated here for better organization and to avoid errors.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mean\n",
        "import warnings\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Preprocessing and Imputation\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.decomposition import PCA\n",
        "from kneed import KneeLocator\n",
        "import miceforest as mf\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Metrics and Evaluation\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031866fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(r'https://raw.githubusercontent.com/avnyadav/sensor-fault-detection/main/aps_failure_training_set1.csv', na_values='na')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "976280e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda_markdown_start",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d4861d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#define numeric and categorical columns\n",
        "numeric_columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]\n",
        "categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "\n",
        "print(\"We have {} numeric columns: {}\".format(len(numeric_columns), numeric_columns))\n",
        "print(\"We have {} categorical columns: {}\".format(len(categorical_columns), categorical_columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec9360d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check and visualize missing values in each column\n",
        "\n",
        "# Calculate missing value percentage\n",
        "missing_values = df.isna().sum().div(df.shape[0]).mul(100).to_frame(name='missing_percent')\n",
        "\n",
        "# Sort in descending order\n",
        "missing_values = missing_values.sort_values(by='missing_percent', ascending=False)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.bar(missing_values.index, missing_values['missing_percent'], color='orange')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Percentage of missing values')\n",
        "plt.title('Missing Values in Each Column')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3bad90f",
      "metadata": {},
      "source": [
        "### Exclude Columns with 70%+ Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de37a3b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "dropcols = missing_values[missing_values['missing_percent'] > 70]\n",
        "# Drop columns with more than 70% missing values\n",
        "dropcols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5656b419",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.drop(columns=dropcols.index, inplace=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8308fde7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< CHANGED SECTION >>>\n",
        "# This cell now correctly calculates the total missing values *after* dropping the highly-null columns.\n",
        "\n",
        "missing_values_count = df.isna().sum()\n",
        "total_cells = np.prod(df.shape)\n",
        "total_missing = missing_values_count.sum()\n",
        "print(f\"Total missing values: {total_missing} out of {total_cells} cells ({(total_missing / total_cells) * 100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b391b4b",
      "metadata": {},
      "source": [
        "### Visualize the Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e35b39f",
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_count = df['class'].value_counts().get('pos', 0)\n",
        "negative_count = df['class'].value_counts().get('neg', 0)\n",
        "print(\"positive: \" + str(positive_count), \", Negative: \" + str(negative_count))\n",
        "sns.catplot(data=df, x='class', kind='count', palette=\"winter_r\", alpha = 0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "helper_funcs_md",
      "metadata": {},
      "source": [
        "## Helper Functions for Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b634365",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_clf(true, predicted):\n",
        "    \"\"\"\n",
        "    This function takes in true values and predicted values\n",
        "    Returns: Accuracy, F1-Score, Precision, Recall, Roc-auc Score\n",
        "    \"\"\"\n",
        "\n",
        "    acc = accuracy_score(true, predicted) # Calculate Accuracy\n",
        "    \n",
        "    f1 = f1_score(true, predicted) # Calculate F I-score\n",
        "    \n",
        "    precision = precision_score(true, predicted) # Calculate Precision\n",
        "   \n",
        "    recall = recall_score(true, predicted) # Calculate Recall\n",
        "   \n",
        "    roc_auc = roc_auc_score(true, predicted) #Calculate Roc\n",
        "    \n",
        "    return acc, f1, precision, recall, roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c52f7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def total_cost(y_true, y_pred):\n",
        "    '''function accepts y_true and y_pred and returns the total cost of misclassification'''\n",
        "    # tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (cm[0,0], 0, 0, 0) if cm.shape == (1,1) else (0,0,0,0) # Handle edge cases\n",
        "    cost = fp*10 + fn*500\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24ec43f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< CHANGED SECTION >>>\n",
        "# Fixed a bug where the function would return after only the first model.\n",
        "# The creation of the report and the return statement are now correctly placed outside the loop.\n",
        "def evaluate_models(X, y, models):\n",
        "    \"\"\"\n",
        "    This function takes in x, y and a list of models\n",
        "    Returns: A dataframe with model name and test performance metrics.\n",
        "    \"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    results_list = []\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)  # train the model\n",
        "        \n",
        "        # make predictions\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        \n",
        "        # Training set performance\n",
        "        model_train_accuracy, model_train_f1, model_train_precision, model_train_recall, model_train_rocauc_score = evaluate_clf(y_train, y_train_pred)\n",
        "        train_cost = total_cost(y_train, y_train_pred)\n",
        "        \n",
        "        # Test set performance\n",
        "        model_test_accuracy, model_test_f1, model_test_precision, model_test_recall, model_test_rocauc_score = evaluate_clf(y_test, y_test_pred)\n",
        "        test_cost = total_cost(y_test, y_test_pred)\n",
        "        \n",
        "        print(f\"Model: {name}\")\n",
        "        print('--- Model performance for Training set ---')\n",
        "        print(f\"- Accuracy: {model_train_accuracy:.4f}\")\n",
        "        print(f\"- F1-Score: {model_train_f1:.4f}\")\n",
        "        print(f\"- Precision: {model_train_precision:.4f}\")\n",
        "        print(f\"- Recall: {model_train_recall:.4f}\")\n",
        "        print(f\"- ROC-AUC Score: {model_train_rocauc_score:.4f}\")\n",
        "        print(f\"- Total Cost: {train_cost:.2f}\")\n",
        "        \n",
        "        print(\"--------------------------------------------------------------\")\n",
        "        \n",
        "        print('--- Model performance for Test set ---')\n",
        "        print(f\"- Accuracy: {model_test_accuracy:.4f}\")\n",
        "        print(f\"- F1-Score: {model_test_f1:.4f}\")\n",
        "        print(f\"- Precision: {model_test_precision:.4f}\")\n",
        "        print(f\"- Recall: {model_test_recall:.4f}\")\n",
        "        print(f\"- ROC-AUC Score: {model_test_rocauc_score:.4f}\")\n",
        "        print(f\"- Total Cost: {test_cost:.2f}\")\n",
        "        print(\"==============================================================\\n\")\n",
        "        \n",
        "        results_list.append({\n",
        "            'Model Name': name,\n",
        "            'Accuracy': model_test_accuracy,\n",
        "            'F1-Score': model_test_f1,\n",
        "            'Precision': model_test_precision,\n",
        "            'Recall': model_test_recall,\n",
        "            'ROC-AUC': model_test_rocauc_score,\n",
        "            'Total Cost': test_cost\n",
        "        })\n",
        "        \n",
        "    report = pd.DataFrame(results_list).sort_values(by='Total Cost', ascending=True)\n",
        "    return report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preprocessing_markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d5d615",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop('class', axis=1)\n",
        "y = df['class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f257785d",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = y.replace({'pos': 1, 'neg': 0})  # Convert target variable to binary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91029da3",
      "metadata": {},
      "source": [
        "### Experiment 1: KNN Imputer with Robust Scaling\n",
        "\n",
        "KNN Imputer is computationally expensive. The imputed and resampled datasets will be saved to disk to avoid re-computation in subsequent runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a0c0cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# The following code for finding the best K for KNNImputer was likely run once.\n",
        "# We will proceed with the chosen K=3 based on the original notebook's implicit choice.\n",
        "# This step is very time-consuming and is shown here for completeness.\n",
        "\n",
        "# results = []\n",
        "# strategies = [str(i) for i in [1,3,5,7,9]]\n",
        "# for s in strategies:\n",
        "#     pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))), ('m',LogisticRegression())])\n",
        "#     scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=2, n_jobs=-1)\n",
        "#     results.append(scores)\n",
        "#     print(f\"n_neighbours = {s} - Accuracy: {mean(scores):.4f} ± {np.std(scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "knn_impute_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the KNN imputed data.\n",
        "\n",
        "KNN_IMPUTED_DATA_PATH = 'X_knn_imputed.pkl'\n",
        "\n",
        "if os.path.exists(KNN_IMPUTED_DATA_PATH):\n",
        "    print(f\"Loading pre-computed data from {KNN_IMPUTED_DATA_PATH}\")\n",
        "    X_knn = joblib.load(KNN_IMPUTED_DATA_PATH)\n",
        "else:\n",
        "    print(\"Performing KNN Imputation (this may take a long time)...\")\n",
        "    knn_pipeline = Pipeline(steps=[\n",
        "        ('imputer', KNNImputer(n_neighbors=3)),\n",
        "        ('RobustScaler', RobustScaler())\n",
        "    ])\n",
        "    X_knn = knn_pipeline.fit_transform(X)\n",
        "    joblib.dump(X_knn, KNN_IMPUTED_DATA_PATH)\n",
        "    print(f\"Saved imputed data to {KNN_IMPUTED_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "knn_resample_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the resampled data.\n",
        "\n",
        "X_RESAMPLED_KNN_PATH = 'X_resampled_knn.pkl'\n",
        "Y_RESAMPLED_KNN_PATH = 'y_resampled_knn.pkl'\n",
        "\n",
        "if os.path.exists(X_RESAMPLED_KNN_PATH) and os.path.exists(Y_RESAMPLED_KNN_PATH):\n",
        "    print(\"Loading pre-resampled KNN data...\")\n",
        "    X_res, y_res = joblib.load(X_RESAMPLED_KNN_PATH), joblib.load(Y_RESAMPLED_KNN_PATH)\n",
        "else:\n",
        "    print(\"Performing SMOTETomek resampling for KNN data...\")\n",
        "    smote_tomek = SMOTETomek(random_state=42, sampling_strategy='minority', n_jobs=-1)\n",
        "    X_res, y_res = smote_tomek.fit_resample(X_knn, y)\n",
        "    joblib.dump(X_res, X_RESAMPLED_KNN_PATH)\n",
        "    joblib.dump(y_res, Y_RESAMPLED_KNN_PATH)\n",
        "    print(\"Saved resampled KNN data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model_definitions",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary which contains models for all experiments\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
        "    \"XGBClassifier\": XGBClassifier(), \n",
        "    \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
        "    \"AdaBoost Classifier\": AdaBoostClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ef6e8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_knn = evaluate_models(X_res, y_res, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e8a4ec5",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_knn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbf44652",
      "metadata": {},
      "source": [
        "### Experiment 2: Simple Imputer with Strategy 'Median'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "median_impute_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the median imputed data.\n",
        "\n",
        "MEDIAN_IMPUTED_DATA_PATH = 'X_median_imputed.pkl'\n",
        "\n",
        "if os.path.exists(MEDIAN_IMPUTED_DATA_PATH):\n",
        "    print(f\"Loading pre-computed data from {MEDIAN_IMPUTED_DATA_PATH}\")\n",
        "    X_median = joblib.load(MEDIAN_IMPUTED_DATA_PATH)\n",
        "else:\n",
        "    print(\"Performing Median Imputation...\")\n",
        "    median_pipeline = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('RobustScaler', RobustScaler())\n",
        "    ])\n",
        "    X_median = median_pipeline.fit_transform(X)\n",
        "    joblib.dump(X_median, MEDIAN_IMPUTED_DATA_PATH)\n",
        "    print(f\"Saved imputed data to {MEDIAN_IMPUTED_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9cce2be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the resampled data.\n",
        "\n",
        "X_RESAMPLED_MEDIAN_PATH = 'X_resampled_median.pkl'\n",
        "Y_RESAMPLED_MEDIAN_PATH = 'y_resampled_median.pkl'\n",
        "\n",
        "if os.path.exists(X_RESAMPLED_MEDIAN_PATH) and os.path.exists(Y_RESAMPLED_MEDIAN_PATH):\n",
        "    print(\"Loading pre-resampled median data...\")\n",
        "    X_res, y_res = joblib.load(X_RESAMPLED_MEDIAN_PATH), joblib.load(Y_RESAMPLED_MEDIAN_PATH)\n",
        "else:\n",
        "    print(\"Performing SMOTETomek resampling for median data...\")\n",
        "    smt = SMOTETomek(random_state=42, sampling_strategy='minority', n_jobs=-1)\n",
        "    X_res, y_res = smt.fit_resample(X_median, y)\n",
        "    joblib.dump(X_res, X_RESAMPLED_MEDIAN_PATH)\n",
        "    joblib.dump(y_res, Y_RESAMPLED_MEDIAN_PATH)\n",
        "    print(\"Saved resampled median data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb6443d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the models\n",
        "report_median = evaluate_models(X_res, y_res, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "median_report_display",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_median"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc50df5",
      "metadata": {},
      "source": [
        "### Experiment 3: MICE Forest for Imputing Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mice_impute_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the MICE imputed data.\n",
        "\n",
        "MICE_IMPUTED_DATA_PATH = 'X_mice_imputed.pkl'\n",
        "\n",
        "if os.path.exists(MICE_IMPUTED_DATA_PATH):\n",
        "    print(f\"Loading pre-computed data from {MICE_IMPUTED_DATA_PATH}\")\n",
        "    X_mice_imputed = joblib.load(MICE_IMPUTED_DATA_PATH)\n",
        "else:\n",
        "    print(\"Performing MICE Imputation (this may take a while)...\")\n",
        "    X_mice = X.copy()\n",
        "    kernel = mf.ImputationKernel(\n",
        "      X_mice,\n",
        "      save_all_iterations=True,\n",
        "      random_state=1989\n",
        "    )\n",
        "    kernel.mice(3) # Run MICE for 3 iterations\n",
        "    X_mice_imputed = kernel.complete_data()\n",
        "    joblib.dump(X_mice_imputed, MICE_IMPUTED_DATA_PATH)\n",
        "    print(f\"Saved imputed data to {MICE_IMPUTED_DATA_PATH}\")\n",
        "\n",
        "# Scale the data after imputation\n",
        "mice_pipeline = Pipeline(steps=[\n",
        "    ('RobustScaler', RobustScaler())\n",
        "])\n",
        "X_mice = mice_pipeline.fit_transform(X_mice_imputed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mice_resample_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the resampled data.\n",
        "\n",
        "X_RESAMPLED_MICE_PATH = 'X_resampled_mice.pkl'\n",
        "Y_RESAMPLED_MICE_PATH = 'y_resampled_mice.pkl'\n",
        "\n",
        "if os.path.exists(X_RESAMPLED_MICE_PATH) and os.path.exists(Y_RESAMPLED_MICE_PATH):\n",
        "    print(\"Loading pre-resampled MICE data...\")\n",
        "    X_res, y_res = joblib.load(X_RESAMPLED_MICE_PATH), joblib.load(Y_RESAMPLED_MICE_PATH)\n",
        "else:\n",
        "    print(\"Performing SMOTETomek resampling for MICE data...\")\n",
        "    smt = SMOTETomek(random_state=42, sampling_strategy='minority', n_jobs=-1)\n",
        "    X_res, y_res = smt.fit_resample(X_mice, y)\n",
        "    joblib.dump(X_res, X_RESAMPLED_MICE_PATH)\n",
        "    joblib.dump(y_res, Y_RESAMPLED_MICE_PATH)\n",
        "    print(\"Saved resampled MICE data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eae88e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the models\n",
        "report_mice = evaluate_models(X_res, y_res, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5daae7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_mice"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193c5bca",
      "metadata": {},
      "source": [
        "### Experiment 4: Simple Imputer with Strategy 'Constant'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "const_impute_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the constant imputed data.\n",
        "\n",
        "CONST_IMPUTED_DATA_PATH = 'X_const_imputed.pkl'\n",
        "\n",
        "if os.path.exists(CONST_IMPUTED_DATA_PATH):\n",
        "    print(f\"Loading pre-computed data from {CONST_IMPUTED_DATA_PATH}\")\n",
        "    X_const = joblib.load(CONST_IMPUTED_DATA_PATH)\n",
        "else:\n",
        "    print(\"Performing Constant Imputation...\")\n",
        "    constant_pipeline = Pipeline(steps=[\n",
        "        ('Imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
        "        ('RobustScaler', RobustScaler())\n",
        "    ])\n",
        "    X_const = constant_pipeline.fit_transform(X)\n",
        "    joblib.dump(X_const, CONST_IMPUTED_DATA_PATH)\n",
        "    print(f\"Saved imputed data to {CONST_IMPUTED_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7b52cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the resampled data.\n",
        "\n",
        "X_RESAMPLED_CONST_PATH = 'X_resampled_const.pkl'\n",
        "Y_RESAMPLED_CONST_PATH = 'y_resampled_const.pkl'\n",
        "\n",
        "if os.path.exists(X_RESAMPLED_CONST_PATH) and os.path.exists(Y_RESAMPLED_CONST_PATH):\n",
        "    print(\"Loading pre-resampled constant data...\")\n",
        "    X_res, y_res = joblib.load(X_RESAMPLED_CONST_PATH), joblib.load(Y_RESAMPLED_CONST_PATH)\n",
        "else:\n",
        "    print(\"Performing SMOTETomek resampling for constant data...\")\n",
        "    smt = SMOTETomek(random_state=42, sampling_strategy='minority', n_jobs=-1)\n",
        "    X_res, y_res = smt.fit_resample(X_const, y)\n",
        "    joblib.dump(X_res, X_RESAMPLED_CONST_PATH)\n",
        "    joblib.dump(y_res, Y_RESAMPLED_CONST_PATH)\n",
        "    print(\"Saved resampled constant data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc0674e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# training the models\n",
        "report_const = evaluate_models(X_res, y_res, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "const_report_display",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_const"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e99b7342",
      "metadata": {},
      "source": [
        "### Experiment 5: Simple Imputer with Strategy 'Mean'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mean_impute_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the mean imputed data.\n",
        "\n",
        "MEAN_IMPUTED_DATA_PATH = 'X_mean_imputed.pkl'\n",
        "\n",
        "if os.path.exists(MEAN_IMPUTED_DATA_PATH):\n",
        "    print(f\"Loading pre-computed data from {MEAN_IMPUTED_DATA_PATH}\")\n",
        "    X_mean = joblib.load(MEAN_IMPUTED_DATA_PATH)\n",
        "else:\n",
        "    print(\"Performing Mean Imputation...\")\n",
        "    mean_pipeline = Pipeline(steps=[\n",
        "        ('Imputer', SimpleImputer(strategy='mean')),\n",
        "        ('RobustScaler', RobustScaler())\n",
        "    ])\n",
        "    X_mean = mean_pipeline.fit_transform(X)\n",
        "    joblib.dump(X_mean, MEAN_IMPUTED_DATA_PATH)\n",
        "    print(f\"Saved imputed data to {MEAN_IMPUTED_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mean_resample_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the resampled data.\n",
        "\n",
        "X_RESAMPLED_MEAN_PATH = 'X_resampled_mean.pkl'\n",
        "Y_RESAMPLED_MEAN_PATH = 'y_resampled_mean.pkl'\n",
        "\n",
        "if os.path.exists(X_RESAMPLED_MEAN_PATH) and os.path.exists(Y_RESAMPLED_MEAN_PATH):\n",
        "    print(\"Loading pre-resampled mean data...\")\n",
        "    X_res, y_res = joblib.load(X_RESAMPLED_MEAN_PATH), joblib.load(Y_RESAMPLED_MEAN_PATH)\n",
        "else:\n",
        "    print(\"Performing SMOTETomek resampling for mean data...\")\n",
        "    smt = SMOTETomek(random_state=42, sampling_strategy='minority', n_jobs=-1)\n",
        "    X_res, y_res = smt.fit_resample(X_mean, y)\n",
        "    joblib.dump(X_res, X_RESAMPLED_MEAN_PATH)\n",
        "    joblib.dump(y_res, Y_RESAMPLED_MEAN_PATH)\n",
        "    print(\"Saved resampled mean data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a8fdb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training all models\n",
        "report_mean = evaluate_models(X_res, y_res, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fde232e",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b21d73",
      "metadata": {},
      "source": [
        "### Experiment 6: Principle Component Analysis (PCA)\n",
        "\n",
        "Using the best imputation method from above (constant fill) and then applying PCA for dimensionality reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5f5136",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data is already imputed and scaled from Experiment 4, stored in X_const\n",
        "X_pca_input = X_const"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1899a82",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Applying PCA to find optimal number of components\n",
        "var_ratio={}\n",
        "for n in range(2,150):\n",
        "    pc=PCA(n_components=n)\n",
        "    df_pca=pc.fit(X_pca_input)\n",
        "    var_ratio[n]=sum(df_pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9cf9a9c",
      "metadata": {},
      "source": [
        "#### Variance Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c3ad49",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotting variance ratio\n",
        "pd.Series(var_ratio).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b244aa",
      "metadata": {},
      "source": [
        "#### K-Need Algorithm to Find the Elbow Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kneed_locator",
      "metadata": {},
      "outputs": [],
      "source": [
        "i = np.arange(len(var_ratio))\n",
        "variance_ratio_list = list(var_ratio.values())\n",
        "components = list(var_ratio.keys())\n",
        "knee = KneeLocator(components, variance_ratio_list, S=1.0, curve='concave', direction='increasing')\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "knee.plot_knee()\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Explained Variance\")\n",
        "plt.show()\n",
        "\n",
        "optimal_k = knee.knee\n",
        "print('Optimal number of components (k) found by KneeLocator:', optimal_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640b8067",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reducing the dimensions of the data based on the optimal k\n",
        "pca_final = PCA(n_components=optimal_k, random_state=42)\n",
        "X_pca_reduced = pca_final.fit_transform(X_pca_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pca_resample_save_load",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< NEW SECTION >>>\n",
        "# Save/Load functionality for the resampled PCA data.\n",
        "\n",
        "X_RESAMPLED_PCA_PATH = 'X_resampled_pca.pkl'\n",
        "Y_RESAMPLED_PCA_PATH = 'y_resampled_pca.pkl'\n",
        "\n",
        "if os.path.exists(X_RESAMPLED_PCA_PATH) and os.path.exists(Y_RESAMPLED_PCA_PATH):\n",
        "    print(\"Loading pre-resampled PCA data...\")\n",
        "    X_res, y_res = joblib.load(X_RESAMPLED_PCA_PATH), joblib.load(Y_RESAMPLED_PCA_PATH)\n",
        "else:\n",
        "    print(\"Performing SMOTETomek resampling for PCA data...\")\n",
        "    smt = SMOTETomek(random_state=42, sampling_strategy='minority', n_jobs=-1)\n",
        "    X_res, y_res = smt.fit_resample(X_pca_reduced, y)\n",
        "    joblib.dump(X_res, X_RESAMPLED_PCA_PATH)\n",
        "    joblib.dump(y_res, Y_RESAMPLED_PCA_PATH)\n",
        "    print(\"Saved resampled PCA data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62297eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training all models on PCA-transformed data\n",
        "report_pca = evaluate_models(X_res, y_res, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0300ef2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "report_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final_summary_md",
      "metadata": {},
      "source": [
        "## Final Results Summary\n",
        "\n",
        "The XGBoost Classifier with Simple Imputation (Constant fill value) provided the lowest total cost, making it the best model for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f8f62a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "pt=PrettyTable()\n",
        "pt.field_names=[\"Model\",\"Imputation_method\",\"Total_cost\"]\n",
        "pt.add_row([\"XGBClassifier\",\"Simple Imputer-Constant\", report_const.loc[report_const['Model Name'] == 'XGBClassifier', 'Total Cost'].iloc[0]])\n",
        "pt.add_row([\"XGBClassifier\",\"Mice\", report_mice.loc[report_mice['Model Name'] == 'XGBClassifier', 'Total Cost'].iloc[0]])\n",
        "pt.add_row([\"XGBClassifier\",\"Knn-Imputer\", report_knn.loc[report_knn['Model Name'] == 'XGBClassifier', 'Total Cost'].iloc[0]])\n",
        "pt.add_row([\"XGBClassifier\",\"Simple Imputer-Mean\", report_mean.loc[report_mean['Model Name'] == 'XGBClassifier', 'Total Cost'].iloc[0]])\n",
        "pt.add_row([\"CatBoostClassifier\",\"Median\", report_median.loc[report_median['Model Name'] == 'CatBoosting Classifier', 'Total Cost'].iloc[0]])\n",
        "pt.add_row([\"Random Forest\",\"PCA\", report_pca.loc[report_pca['Model Name'] == 'Random Forest', 'Total Cost'].iloc[0]])\n",
        "print(pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final_model_md",
      "metadata": {},
      "source": [
        "## Final Model Training (Best Performing Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e58e102a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We will use the resampled data from the best experiment (Constant Imputation)\n",
        "X_final, y_final = joblib.load(X_RESAMPLED_CONST_PATH), joblib.load(Y_RESAMPLED_CONST_PATH)\n",
        "\n",
        "final_model = XGBClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cdc3b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)\n",
        "\n",
        "final_model = final_model.fit(X_train, y_train)\n",
        "y_pred = final_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d50dd81",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Final XGBoost Classifier Accuracy Score (Train) :\", final_model.score(X_train,y_train))\n",
        "print(\"Final XGBoost Classifier Accuracy Score (Test) :\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57e2583c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Final XGBoost Classifier Cost Metric(Test) :\",total_cost(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92980bf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<< CHANGED SECTION >>>\n",
        "# `plot_confusion_matrix` is deprecated. Using `ConfusionMatrixDisplay.from_estimator` instead.\n",
        "\n",
        "# plots Confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ConfusionMatrixDisplay.from_estimator(final_model, X_test, y_test, cmap='Blues', values_format='d', ax=ax)\n",
        "plt.title('Confusion Matrix for Final XGBoost Model')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
